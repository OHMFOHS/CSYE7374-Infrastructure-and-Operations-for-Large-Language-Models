{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24d716d5-9881-46ae-abb7-0c408ee1e08a",
   "metadata": {},
   "source": [
    "# Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e4dfe30-1a62-488c-bc8a-e8c4f1343cac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/CSYE/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset # Hugging Face library for accessing and streaming large public text datasets\n",
    "from transformers import AutoTokenizer # Provides transformer-compatible tokenizers\n",
    "from torch.utils.data import Dataset, DataLoader # PyTorch utilities for building custom datasets and data loaders\n",
    "from torch.nn.utils.rnn import pad_sequence # Function to pad variable-length sequences in a batch to the same length\n",
    "from tqdm import tqdm # Progress-bar library for displaying iteration progress in loops\n",
    "import torch, regex as re, unicodedata # PyTorch core library\n",
    "# regex: advanced regular-expression library used for text cleaning and normalization\n",
    "# unicodedata: handles Unicode normalization to ensure consistent text representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a588fad-8927-4bc8-8445-e13edb839ad2",
   "metadata": {},
   "source": [
    "# Dataset collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a1b313f-2c7e-4a2a-a452-5a86210da6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki = load_dataset(\"wikipedia\", \"20220301.en\", split=\"train\", streaming=True)\n",
    "# Load the English Wikipedia dataset (March 2022 dump) from the Hugging Face Hub.\n",
    "owt  = load_dataset(\"openwebtext\", split=\"train\", streaming=True)\n",
    "# Load the OpenWebText dataset â€” a large collection of web pages\n",
    "texts = []\n",
    "# Create an empty list to hold text samples collected from both datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0951f28e-4c04-49a3-a58f-c397241b8e8e",
   "metadata": {},
   "source": [
    "# Collect small sample from each source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e959ac74-3baa-429d-b92d-3ec62c2f6419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded 2000 documents (streaming mode, no disk write)\n"
     ]
    }
   ],
   "source": [
    "for i, t in enumerate(wiki):\n",
    "    texts.append(t[\"text\"])\n",
    "    if i >= 999:\n",
    "        break\n",
    "for i, t in enumerate(owt):\n",
    "    texts.append(t[\"text\"])\n",
    "    if i >= 999:\n",
    "        break\n",
    "#extract main part of article as plain string, add it to texts\n",
    "#take the first 1000 samples for demonstration purposes.\n",
    "print(f\"âœ… Loaded {len(texts)} documents (streaming mode, no disk write)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e807cd85-6711-4672-9568-62720a620802",
   "metadata": {},
   "source": [
    "# Cleaning and normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1698c365-5248-40ed-a3b0-5e8e4d59b53d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… After cleaning: 1985 unique documents\n"
     ]
    }
   ],
   "source": [
    "#Normalize and clean a raw text string.\n",
    "def clean_text(t: str) -> str:\n",
    "    \"\"\"Normalize text: lowercase, remove HTML, extra whitespace, long repeats.\"\"\"\n",
    "    t = unicodedata.normalize(\"NFKC\", t)    # normalize unicode form\n",
    "    t = t.lower()                           # convert to lowercase\n",
    "    t = re.sub(r\"<[^>]+>\", \" \", t)          # remove HTML tags\n",
    "    t = re.sub(r\"\\s+\", \" \", t).strip()      # collapse spaces\n",
    "    t = re.sub(r\"(.)\\1{4,}\", r\"\\1\\1\\1\", t)  # compress long repeats\n",
    "    return t\n",
    "\n",
    "#Initialize an empty list for cleaned documents, and a set to keep track of seen texts\n",
    "cleaned, seen = [], set()\n",
    "\n",
    "for txt in texts:\n",
    "    # Skip empty or very short documents (< 50 words)\n",
    "    if not txt or len(txt.split()) < 50:\n",
    "        continue\n",
    "    txt = clean_text(txt)\n",
    "    if txt in seen:\n",
    "        continue\n",
    "    seen.add(txt)\n",
    "    cleaned.append(txt)\n",
    "\n",
    "print(f\"âœ… After cleaning: {len(cleaned)} unique documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4b567e-716c-4ccb-a962-856a462dc81b",
   "metadata": {},
   "source": [
    "# Tokenization and chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1036fc45-ee7e-4a1b-999b-dc2225b35933",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing:   0%|                                      | 0/1985 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (8144 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Tokenizing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1985/1985 [00:08<00:00, 229.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Created 49302 tokenized chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load a pretrained GPT-2 tokenizer from the Hugging Face Transformers library.\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Padding tokens are needed when batching sequences of different lengths.\n",
    "# Here we assign the padding token to be the same as the EOS\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "# Define the maximum sequence length\n",
    "block_size = 128\n",
    "# Initialize an empty list to store tokenized blocks (lists of token IDs).\n",
    "tokenized_blocks = []\n",
    "\n",
    "# Iterate through each cleaned document and tokenize it.\n",
    "# tqdm provides a progress bar for monitoring the tokenization process.\n",
    "for t in tqdm(cleaned, desc=\"Tokenizing\"):\n",
    "    # Convert text to a list of integer token IDs.\n",
    "    ids = tokenizer.encode(t, add_special_tokens=False)\n",
    "\n",
    "    # Append the EOS token ID at the end of each document, to indicate the end of a sequence.\n",
    "    ids.append(tokenizer.eos_token_id)\n",
    "\n",
    "    # If a document is longer than block_size, we split it into consecutive chunks of equal length\n",
    "    # This ensures each training sample fits within the model's maximum context window.\n",
    "    for i in range(0, len(ids), block_size):\n",
    "        tokenized_blocks.append(ids[i:i+block_size])\n",
    "\n",
    "print(f\"âœ… Created {len(tokenized_blocks)} tokenized chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628ebe63-18bf-444b-9b50-218d1a046bc4",
   "metadata": {},
   "source": [
    "# Custom PyTorch DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21cba9c1-1122-41a3-b53a-7e70b55a99ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 shapes: torch.Size([4, 128])\n",
      "Batch 1 shapes: torch.Size([4, 128])\n",
      "Batch 2 shapes: torch.Size([4, 128])\n",
      "Batch 3 shapes: torch.Size([4, 128])\n",
      "Batch 4 shapes: torch.Size([4, 128])\n",
      "Batch 5 shapes: torch.Size([4, 128])\n"
     ]
    }
   ],
   "source": [
    "# Define a simple PyTorch Dataset to wrap tokenized text data\n",
    "# Each item in the dataset corresponds to one tokenized block (list of token IDs).\n",
    "class TextDataset(Dataset):\n",
    "    \"\"\"Simple tokenized dataset for pretraining.\"\"\"\n",
    "    # Store all tokenized blocks inside the dataset.\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "    # Return the total number of samples (blocks) in the dataset.\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "     # Retrieve a tokenized block by its index.\n",
    "    def __getitem__(self, idx):\n",
    "        ids = torch.tensor(self.data[idx], dtype=torch.long)\n",
    "        return {\"input_ids\": ids, \"labels\": ids.clone()}\n",
    "\n",
    "# Define a custom collate function to handle variable-length sequences in each batch\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Pad variable-length sequences in a batch.\"\"\"\n",
    "    # Extract the 'input_ids' tensors from all samples in this batch.\n",
    "    ids = [b[\"input_ids\"] for b in batch]\n",
    "    # Use PyTorch's pad_sequence() to pad shorter sequences, so that all sequences in the batch have the same length.\n",
    "    padded = pad_sequence(ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    return {\"input_ids\": padded, \"labels\": padded.clone()}\n",
    "\n",
    "# Initialize the custom Dataset using the tokenized data blocks.\n",
    "dataset = TextDataset(tokenized_blocks)\n",
    "# Create a DataLoader to generate mini-batches for training.\n",
    "loader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# Iterate through the DataLoader to visualize the batch shapes.\n",
    "\n",
    "for i, batch in enumerate(loader):\n",
    "    # Print the shape of 'input_ids' for each batch.\n",
    "    print(f\"Batch {i} shapes: {batch['input_ids'].shape}\")\n",
    "    if i == 5:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e120425b-366e-4c24-b92d-e149dae347f8",
   "metadata": {},
   "source": [
    "# Save sample processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d89ebae4-b6b1-4728-808e-77f6e58be098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved sample batch to sample_dataset.pt\n",
      "ðŸŽ‰ All steps completed successfully (streaming version ready for submission)!\n"
     ]
    }
   ],
   "source": [
    "for batch in loader:\n",
    "    torch.save(batch, \"sample_dataset.pt\")\n",
    "    print(\"âœ… Saved sample batch to sample_dataset.pt\")\n",
    "    break\n",
    "\n",
    "print(\"ðŸŽ‰ All steps completed successfully (streaming version ready for submission)!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
