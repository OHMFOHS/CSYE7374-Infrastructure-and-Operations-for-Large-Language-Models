{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44bc8ab7-5560-4254-9316-e2455c248245",
   "metadata": {},
   "source": [
    "# 1. Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c790c5c-51fc-49e3-8310-301011870e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset # Hugging Face library for accessing and streaming large public text datasets\n",
    "from transformers import AutoTokenizer # Provides transformer-compatible tokenizers\n",
    "from torch.utils.data import Dataset, DataLoader # PyTorch utilities for building custom datasets and data loaders\n",
    "from torch.nn.utils.rnn import pad_sequence # Function to pad variable-length sequences in a batch to the same length\n",
    "from tqdm import tqdm # Progress-bar library for displaying iteration progress in loops\n",
    "import torch, regex as re, unicodedata # PyTorch core library\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "# regex: advanced regular-expression library used for text cleaning and normalization\n",
    "# unicodedata: handles Unicode normalization to ensure consistent text representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed51b3ec-1899-496c-9874-779ed978bc17",
   "metadata": {},
   "source": [
    "# 2. Dataset processing pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444232c0-933c-4549-8110-fffe2c623cb0",
   "metadata": {},
   "source": [
    "## Collect Data from hugging face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10d01330-5c5f-478a-a328-5e343331703c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki = load_dataset(\"wikipedia\", \"20220301.en\", split=\"train\", streaming=True)\n",
    "# Load the English Wikipedia dataset (March 2022 dump) from the Hugging Face Hub.\n",
    "owt  = load_dataset(\"openwebtext\", split=\"train\", streaming=True)\n",
    "# Load the OpenWebText dataset — a large collection of web pages\n",
    "texts = []\n",
    "# Create an empty list to hold text samples collected from both datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3a191a-728a-47f3-bc3b-c1d356f496ea",
   "metadata": {},
   "source": [
    "## Collect sample from each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "515e5e04-d50c-4e1a-927d-175c64dbcb0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get 2000 documents\n"
     ]
    }
   ],
   "source": [
    "for i,t in enumerate(wiki):\n",
    "    texts.append(t[\"text\"])\n",
    "    if i >= 999:\n",
    "        break\n",
    "for i,t in enumerate(owt):\n",
    "    texts.append(t[\"text\"])\n",
    "    if i >= 999:\n",
    "        break\n",
    "print(f\"get {len(texts)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd72040-3b3a-4b83-982d-8f439abd2165",
   "metadata": {},
   "source": [
    "## Cleaning and Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e70bfa2b-93d2-41d8-98e2-98dae039b679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ After cleaning: 1985 unique documents\n"
     ]
    }
   ],
   "source": [
    "#Normalize and clean a raw text string.\n",
    "def clean_text(t: str) -> str:\n",
    "    \"\"\"Normalize text: lowercase, remove HTML, extra whitespace, long repeats.\"\"\"\n",
    "    t = unicodedata.normalize(\"NFKC\", t)    # normalize unicode form\n",
    "    t = t.lower()                           # convert to lowercase\n",
    "    t = re.sub(r\"<[^>]+>\", \" \", t)          # remove HTML tags\n",
    "    t = re.sub(r\"\\s+\", \" \", t).strip()      # collapse spaces\n",
    "    t = re.sub(r\"(.)\\1{4,}\", r\"\\1\\1\\1\", t)  # compress long repeats\n",
    "    return t\n",
    "\n",
    "#Initialize an empty list for cleaned documents, and a set to keep track of seen texts\n",
    "cleaned, seen = [], set()\n",
    "\n",
    "for txt in texts:\n",
    "    # Skip empty or very short documents (< 50 words)\n",
    "    if not txt or len(txt.split()) < 50:\n",
    "        continue\n",
    "    txt = clean_text(txt)\n",
    "    if txt in seen:\n",
    "        continue\n",
    "    seen.add(txt)\n",
    "    cleaned.append(txt)\n",
    "print(f\"✅ After cleaning: {len(cleaned)} unique documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb376a7-fe58-4ea0-b244-99057457bf2c",
   "metadata": {},
   "source": [
    "## Tokenization and chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "180396c1-22d7-4890-b27d-484c68d8ccfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing:   0%|          | 0/1985 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (8144 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Tokenizing: 100%|██████████| 1985/1985 [00:08<00:00, 221.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Created 49302 tokenized chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load a pretrained GPT-2 tokenizer from the Hugging Face Transformers library.\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Padding tokens are needed when batching sequences of different lengths.\n",
    "# Here we assign the padding token to be the same as the EOS\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Define the maximum sequence length\n",
    "block_size = 128\n",
    "# Initialize an empty list to store tokenized blocks (lists of token IDs).\n",
    "tokenized_blocks = []\n",
    "\n",
    "# Iterate through each cleaned document and tokenize it.\n",
    "# tqdm provides a progress bar for monitoring the tokenization process.\n",
    "for t in tqdm(cleaned, desc=\"Tokenizing\"):\n",
    "    # Convert text to a list of integer token IDs.\n",
    "    ids = tokenizer.encode(t, add_special_tokens=False)\n",
    "\n",
    "    # Append the EOS token ID at the end of each document, to indicate the end of a sequence.\n",
    "    ids.append(tokenizer.eos_token_id)\n",
    "\n",
    "    # If a document is longer than block_size, we split it into consecutive chunks of equal length\n",
    "    # This ensures each training sample fits within the model's maximum context window.\n",
    "    for i in range(0, len(ids), block_size):\n",
    "        tokenized_blocks.append(ids[i:i+block_size])\n",
    "\n",
    "print(f\"✅ Created {len(tokenized_blocks)} tokenized chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0fd2cd-eda4-4c9c-9d4f-de69f8cb00d8",
   "metadata": {},
   "source": [
    "## Split to train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd9c0f30-f44c-4cb4-acfc-4eb366f9a5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train/val\n",
    "split = int(0.9 * len(tokenized_blocks))\n",
    "train_ids = tokenized_blocks[:split]\n",
    "val_ids = tokenized_blocks[split:]\n",
    "\n",
    "# ✅ Save them\n",
    "np.save(\"train.npy\", np.array(train_ids, dtype=object), allow_pickle=True)\n",
    "np.save(\"val.npy\",   np.array(val_ids,   dtype=object), allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6276a4b1-d752-4864-8477-678e020323a6",
   "metadata": {},
   "source": [
    "# 3. Model Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae17ba6-a0ea-42be-857c-2002ae6ae7fc",
   "metadata": {},
   "source": [
    "## Build mini-GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8157cd99-f4c9-4f0b-9021-0926126ab5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniGPT(nn.Module):\n",
    "    def __init__(self, vocab_size, block_size=128, d_model=128, n_heads=4, n_layers=2):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_emb = nn.Embedding(block_size, d_model)\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(d_model, n_heads, 4*d_model, activation='gelu')\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        self.ln = nn.LayerNorm(d_model)\n",
    "        self.head = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T = x.shape\n",
    "        pos = torch.arange(T, device=x.device)\n",
    "        x = self.tok_emb(x) + self.pos_emb(pos)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = self.ln(x)\n",
    "        logits = self.head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d768c6f-ed97-4a65-a843-6f99d8f5ca0b",
   "metadata": {},
   "source": [
    "## Load Data for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ae6b3148-4819-4379-9ad3-3ae3b53f1932",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Custom dataset class\n",
    "class TokenDataset(Dataset):\n",
    "    def __init__(self, blocks):\n",
    "        # Convert each token list to a torch tensor\n",
    "        self.blocks = [torch.tensor(b, dtype=torch.long) for b in blocks]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.blocks)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        x = self.blocks[i][:-1]  # input tokens\n",
    "        y = self.blocks[i][1:]   # next-token targets\n",
    "        return x, y\n",
    "\n",
    "\n",
    "# ✅ Custom collate function for padding\n",
    "def collate_batch(batch):\n",
    "    xs, ys = zip(*batch)\n",
    "    pad_id = tokenizer.pad_token_id  # pad with eos_token if needed\n",
    "    xs = pad_sequence(xs, batch_first=True, padding_value=pad_id)\n",
    "    ys = pad_sequence(ys, batch_first=True, padding_value=pad_id)\n",
    "    return xs, ys\n",
    "\n",
    "\n",
    "# ✅ Load saved tokenized data\n",
    "train_ids = np.load(\"train.npy\", allow_pickle=True)\n",
    "val_ids   = np.load(\"val.npy\", allow_pickle=True)\n",
    "\n",
    "# ✅ Use collate_fn here!\n",
    "train_loader = DataLoader(\n",
    "    TokenDataset(train_ids),\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_batch\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    TokenDataset(val_ids),\n",
    "    batch_size=32,\n",
    "    collate_fn=collate_batch\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd99658f-c474-40d1-a149-8554bca3cb29",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f7d1ef88-a511-4f66-b51a-6ea4c2466fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  11%|█         | 149/1387 [01:46<14:44,  1.40it/s, batch_loss=7.3124]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 41\u001b[0m\n\u001b[1;32m     39\u001b[0m x, y \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     40\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 41\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(\n\u001b[1;32m     43\u001b[0m     logits\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, logits\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)),\n\u001b[1;32m     44\u001b[0m     y\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m     45\u001b[0m     ignore_index\u001b[38;5;241m=\u001b[39mpad_token_id\n\u001b[1;32m     46\u001b[0m )\n\u001b[1;32m     47\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/CSYE/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/CSYE/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[15], line 18\u001b[0m, in \u001b[0;36mMiniGPT.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     16\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtok_emb(x) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_emb(pos)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 18\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln(x)\n\u001b[1;32m     20\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead(x)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/CSYE/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/CSYE/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/CSYE/lib/python3.8/site-packages/torch/nn/modules/transformer.py:750\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[0;34m(self, src, src_mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    749\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sa_block(x, src_mask, src_key_padding_mask, is_causal\u001b[38;5;241m=\u001b[39mis_causal))\n\u001b[0;32m--> 750\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ff_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/opt/anaconda3/envs/CSYE/lib/python3.8/site-packages/torch/nn/modules/transformer.py:765\u001b[0m, in \u001b[0;36mTransformerEncoderLayer._ff_block\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_ff_block\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 765\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear2(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactivation\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    766\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout2(x)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/CSYE/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/CSYE/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/CSYE/lib/python3.8/site-packages/torch/nn/modules/dropout.py:59\u001b[0m, in \u001b[0;36mDropout.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/CSYE/lib/python3.8/site-packages/torch/nn/functional.py:1295\u001b[0m, in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[1;32m   1294\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1295\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39mdropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import math\n",
    "from tqdm import tqdm  # ✅ progress bar library\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "model = MiniGPT(vocab_size=tokenizer.vocab_size).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4)\n",
    "\n",
    "# ✅ Evaluation loop with progress bar\n",
    "def evaluate(loader):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for x, y in tqdm(loader, desc=\"Evaluating\", leave=False):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits = model(x)\n",
    "            loss = F.cross_entropy(\n",
    "                logits.view(-1, logits.size(-1)),\n",
    "                y.view(-1),\n",
    "                ignore_index=pad_token_id\n",
    "            )\n",
    "            losses.append(loss.item())\n",
    "    return sum(losses) / len(losses)\n",
    "\n",
    "\n",
    "# ✅ Training loop with progress bar\n",
    "train_losses, val_losses = [], []\n",
    "epochs = 5\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total = 0\n",
    "\n",
    "    # tqdm progress bar for each epoch\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\", leave=True)\n",
    "    for x, y in progress_bar:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x)\n",
    "        loss = F.cross_entropy(\n",
    "            logits.view(-1, logits.size(-1)),\n",
    "            y.view(-1),\n",
    "            ignore_index=pad_token_id\n",
    "        )\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total += loss.item()\n",
    "\n",
    "        # ✅ show running loss in progress bar\n",
    "        progress_bar.set_postfix({\"batch_loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "    # ✅ evaluate after each epoch\n",
    "    avg_train = total / len(train_loader)\n",
    "    avg_val = evaluate(val_loader)\n",
    "    ppl = math.exp(avg_val)\n",
    "\n",
    "    print(f\"\\nEpoch {epoch+1}: Train={avg_train:.4f}, Val={avg_val:.4f}, PPL={ppl:.2f}\")\n",
    "    torch.save(model.state_dict(), f\"mini_gpt_epoch{epoch+1}.pt\")\n",
    "\n",
    "    train_losses.append(avg_train)\n",
    "    val_losses.append(avg_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562d93af-e941-49e3-994a-1c81fb8160be",
   "metadata": {},
   "source": [
    "# Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c34ff7-cc36-4072-a7e4-0a74d209d4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt, numpy as np\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Val Loss')\n",
    "plt.legend(); plt.title('Training vs Validation Loss'); plt.show()\n",
    "\n",
    "plt.plot(np.exp(val_losses)); plt.title('Validation Perplexity'); plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
